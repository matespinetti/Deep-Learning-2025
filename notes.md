# üìã Gu√≠a de Preprocesamiento de Datos

## 1. üîç EXPLORACI√ìN INICIAL
- [ ] **Cargar y visualizar los datos**
  - [ ] `df.head()`, `df.tail()`, `df.info()`
  - [ ] `df.describe()` para estad√≠sticas b√°sicas
  - [ ] `df.shape` para dimensiones
- [ ] **Identificar tipos de variables**
  - [ ] Cuantitativas (continuas/discretas)
  - [ ] Cualitativas (nominales/ordinales)
- [ ] **Verificar valores √∫nicos**
  - [ ] `df.nunique()` por columna
  - [ ] `df.value_counts()` para categ√≥ricas

## 2. üö® DETECCI√ìN DE PROBLEMAS
- [ ] **Valores faltantes**
  - [ ] `df.isnull().sum()` - cantidad por columna
  - [ ] `df.isnull().sum() / len(df) * 100` - porcentaje
  - [ ] Visualizar con `sns.heatmap(df.isnull())`
- [ ] **Valores duplicados**
  - [ ] `df.duplicated().sum()`
  - [ ] `df.drop_duplicates()` si es necesario
- [ ] **Outliers**
  - [ ] Boxplots para variables num√©ricas
  - [ ] `sns.boxplot(data=df, y='variable')`
  - [ ] Z-score o IQR para detectar

## 3. üßπ LIMPIEZA DE DATOS
- [ ] **Manejar valores faltantes**
  - [ ] Eliminar: `df.dropna()` (si pocos casos)
  - [ ] Imputar: `df.fillna()` o `SimpleImputer`
  - [ ] Crear categor√≠a: `df['col'].fillna('Unknown')`
- [ ] **Eliminar outliers** (si es necesario)
  - [ ] IQR method: `Q1 - 1.5*IQR` y `Q3 + 1.5*IQR`
  - [ ] Z-score: `|z| > 3`
- [ ] **Estandarizar formatos**
  - [ ] Fechas: `pd.to_datetime()`
  - [ ] Texto: `.str.lower()`, `.str.strip()`
  - [ ] Categor√≠as: `.str.capitalize()`

## 4. üîÑ TRANSFORMACI√ìN DE VARIABLES
- [ ] **Variables categ√≥ricas**
  - [ ] **Nominales**: One-Hot Encoding (`pd.get_dummies()` o `OneHotEncoder`)
  - [ ] **Ordinales**: Label Encoding (`LabelEncoder` o `OrdinalEncoder`)
  - [ ] **Binarias**: Mapeo directo (`{'yes': 1, 'no': 0}`)
- [ ] **Variables num√©ricas**
  - [ ] **Discretas**: Verificar si necesitan transformaci√≥n
  - [ ] **Continuas**: Estandarizaci√≥n/normalizaci√≥n
- [ ] **Crear nuevas variables**
  - [ ] Binning: `pd.cut()`, `pd.qcut()`
  - [ ] Interacciones: `df['A'] * df['B']`
  - [ ] Transformaciones: `np.log()`, `np.sqrt()`

## 5. üìä NORMALIZACI√ìN/ESTANDARIZACI√ìN
- [ ] **Elegir m√©todo apropiado**
  - [ ] **StandardScaler**: `(x - Œº) / œÉ` (media=0, std=1)
  - [ ] **MinMaxScaler**: `(x - min) / (max - min)` (rango [0,1])
  - [ ] **RobustScaler**: usa mediana e IQR (resistente a outliers)
- [ ] **Aplicar SOLO a variables num√©ricas**
- [ ] **Verificar distribuci√≥n despu√©s**

## 6. ‚úÇÔ∏è DIVISI√ìN DE DATOS
- [ ] **Dividir ANTES de estandarizar**
  - [ ] `train_test_split(X, y, test_size=0.2, random_state=42)`
  - [ ] Validaci√≥n adicional si es necesario
- [ ] **Verificar balance de clases**
  - [ ] `y_train.value_counts()`
  - [ ] Estratificar si hay desbalance: `stratify=y`

## 7. üéØ APLICAR TRANSFORMACIONES
- [ ] **Estandarizar usando SOLO train**
  - [ ] `scaler.fit(X_train)`
  - [ ] `X_train_scaled = scaler.transform(X_train)`
  - [ ] `X_test_scaled = scaler.transform(X_test)`
- [ ] **Aplicar encoding usando SOLO train**
  - [ ] `encoder.fit(X_train)`
  - [ ] `X_train_encoded = encoder.transform(X_train)`
  - [ ] `X_test_encoded = encoder.transform(X_test)`

## 8. ‚úÖ VALIDACI√ìN FINAL
- [ ] **Verificar dimensiones**
  - [ ] `X_train.shape`, `X_test.shape`
  - [ ] `y_train.shape`, `y_test.shape`
- [ ] **Verificar tipos de datos**
  - [ ] `X_train.dtypes`
  - [ ] No hay valores faltantes: `X_train.isnull().sum()`
- [ ] **Verificar distribuciones**
  - [ ] Comparar train vs test
  - [ ] `sns.histplot()` para visualizar
- [ ] **Guardar transformadores**
  - [ ] `joblib.dump(scaler, 'scaler.pkl')`
  - [ ] `joblib.dump(encoder, 'encoder.pkl')`

## 9. üìà AN√ÅLISIS ADICIONAL
- [ ] **Matriz de correlaci√≥n**
  - [ ] `corr_matrix = X_train.corr()`
  - [ ] `sns.heatmap(corr_matrix)`
- [ ] **Selecci√≥n de caracter√≠sticas**
  - [ ] Eliminar variables altamente correlacionadas
  - [ ] Feature importance si es necesario
- [ ] **Documentar decisiones**
  - [ ] ¬øPor qu√© eliminaste ciertas variables?
  - [ ] ¬øQu√© m√©todo de imputaci√≥n usaste?
  - [ ] ¬øPor qu√© elegiste ese scaler?

## 10. üöÄ PREPARACI√ìN PARA MODELADO
- [ ] **Verificar que todo est√© listo**
  - [ ] Datos limpios y transformados
  - [ ] Sin valores faltantes
  - [ ] Tipos de datos correctos
  - [ ] Dimensiones consistentes
- [ ] **Backup de datos originales**
  - [ ] Guardar versi√≥n sin procesar
  - [ ] Documentar todos los pasos

---

## üí° TIPS IMPORTANTES:

1. **Siempre divide ANTES de estandarizar**
2. **Usa SOLO datos de train para calcular par√°metros**
3. **Documenta cada decisi√≥n de preprocesamiento**
4. **Guarda los transformadores para usar en producci√≥n**
5. **Verifica que no haya data leakage**
6. **Visualiza antes y despu√©s de cada transformaci√≥n**

---

## üìù C√ìDIGO DE EJEMPLO:

```python
# 1. Exploraci√≥n
df.head()
df.info()
df.describe()

# 2. Detecci√≥n de problemas
df.isnull().sum()
df.duplicated().sum()

# 3. Divisi√≥n ANTES de estandarizar
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Estandarizaci√≥n usando SOLO train
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Encoding categ√≥ricas
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
X_train_encoded = encoder.fit_transform(X_train_categorical)
X_test_encoded = encoder.transform(X_test_categorical)
```
